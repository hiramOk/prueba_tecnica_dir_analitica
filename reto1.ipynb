{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPQldfSfyy6M7iAz4UQT1vV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install torch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-Q8WTxpnG1fn","executionInfo":{"status":"ok","timestamp":1678459384387,"user_tz":360,"elapsed":4369,"user":{"displayName":"Hiram Paulin","userId":"14403235101019868012"}},"outputId":"e2497899-fba0-4b10-99c7-f3367c116d63"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch) (4.5.0)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import re\n","import json \n","import string\n","import torch\n","import nltk\n","import gensim\n","from nltk import WordNetLemmatizer\n","from sklearn.metrics import accuracy_score, average_precision_score\n","from sklearn.model_selection import train_test_split\n","import torch.nn as nn\n","import torchtext\n","from torchtext.data import get_tokenizer\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qknDe-3MEUFE","executionInfo":{"status":"ok","timestamp":1678459396129,"user_tz":360,"elapsed":11756,"user":{"displayName":"Hiram Paulin","userId":"14403235101019868012"}},"outputId":"20f515b2-a753-49bd-eb6e-1c887610bbc7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","source":["# Carga de datos\n","Conjunto de datos para entrenar el modelo, con respuestas básicas"],"metadata":{"id":"XVr_RXQYIe4P"}},{"cell_type":"code","source":["f = open('/content/Intent.json')\n","intents = json.load(f)"],"metadata":{"id":"0B2vPQtqGy3f","executionInfo":{"status":"ok","timestamp":1678459427743,"user_tz":360,"elapsed":120,"user":{"displayName":"Hiram Paulin","userId":"14403235101019868012"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# Identificación de variable objetivo y características (preprocesamiento)\n","Los datos tienen diferentes _intenciones_ y respuestas de acuerdo a la intención"],"metadata":{"id":"5xTuIvNkb9hW"}},{"cell_type":"code","source":["def preprocesamiento(line):\n","    line = re.sub(r'[^a-zA-z.?!\\']', ' ', line)\n","    line = re.sub(r'[ ]+', ' ', line)\n","    return line"],"metadata":{"id":"Lv1II3KK8q1M","executionInfo":{"status":"ok","timestamp":1678459429601,"user_tz":360,"elapsed":144,"user":{"displayName":"Hiram Paulin","userId":"14403235101019868012"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# listas para las palabras tokenizadas, las clases que identifican a cada respuesta, las características y la variable objetivo, en ese orden respectivamente\n","w = {}\n","clases = []\n","X = []\n","y = []\n","\n","for intent in intents['intents']:\n","  if intent['intent'] not in clases:\n","        clases.append(intent['intent'])\n","  if intent['intent'] not in w:\n","      w[intent['intent']] = []\n","      \n","  for text in intent['text']:\n","      X.append(preprocesamiento(text))\n","      y.append(intent['intent'])\n","      \n","  for response in intent['responses']:\n","      w[intent['intent']].append(response)\n","\n","# w = [lemmatizer.lemmatize(word.lower()) for word in w if word not in string.punctuation]\n","# w = sorted(set(w))\n","# clases = sorted(set(clases))"],"metadata":{"id":"p_oBDEO3IXsm","executionInfo":{"status":"ok","timestamp":1678461688455,"user_tz":360,"elapsed":160,"user":{"displayName":"Hiram Paulin","userId":"14403235101019868012"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# Creación de la representación del lenguaje\n","De a cuerdo a las restricciones del reto, este modelo es desde cero, por lo tanto usamos una bolsa de palabras(BOW), una técnica de representación de texto que crea un vector con las frecuencias de las palabras"],"metadata":{"id":"0HknwwfnccTX"}},{"cell_type":"code","source":["training = []\n","lista = [0] * len(clases)\n","lemmatizer = WordNetLemmatizer()\n","\n","for i, doc in enumerate(X):\n","  bow = []\n","  txt = lemmatizer.lemmatize(doc.lower())\n","  # print(txt)\n","  for word in w:\n","    bow.append(1) if word not in txt else bow.append(0)\n","\n","  out = list(lista)\n","  out[clases.index(y[i])] = 1\n","  training.append([bow, out])\n","\n","np.random.shuffle(training)\n","training = np.array(training, dtype=object)\n","\n","X_train = torch.FloatTensor(list(training[:, 0]))\n","y_train = torch.FloatTensor(list(training[:, 1]))"],"metadata":{"id":"-Km01PoRXP5x","executionInfo":{"status":"ok","timestamp":1678461744452,"user_tz":360,"elapsed":150,"user":{"displayName":"Hiram Paulin","userId":"14403235101019868012"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["X_train.shape, y_train.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZE7AfbDLLFv","executionInfo":{"status":"ok","timestamp":1678461760117,"user_tz":360,"elapsed":11,"user":{"displayName":"Hiram Paulin","userId":"14403235101019868012"}},"outputId":"9eabfd3d-f688-4529-f42a-c92386ec6dc3"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(torch.Size([143, 22]), torch.Size([143, 22]))"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# red con una capa oculta y dropout para evitar overfitting\n","class NeuralNetwork(torch.nn.Module):\n","  def __init__(self):\n","    super(NeuralNetwork, self).__init__()\n","    self.layers = nn.Sequential(\n","      nn.Linear(len(X_train[0]), 128),\n","      nn.Dropout(0.5),\n","      nn.ReLU(),\n","      nn.Linear(128, 22),\n","      nn.Dropout(0.5),\n","      nn.Softmax()\n","    )\n","\n","  def forward(self, X):\n","    return self.layers(X)"],"metadata":{"id":"OYAPgwj5nzTI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mlp = NeuralNetwork()"],"metadata":{"id":"Q19IdV0p0m5g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss =  nn.CrossEntropyLoss()"],"metadata":{"id":"I236m4ZYuQSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(mlp.parameters(), lr=0.01)"],"metadata":{"id":"jmdokt2gxVWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# entrenamiento de la red neuronal\n","for i in range(1000):\n","  output = mlp(X_train)\n","  costo = loss(output, y_train)\n","  optimizer.zero_grad() # reiniciar el gradiente del optimizador\n","  costo.backward() # gradiente de la función de costo\n","  optimizer.step()\n","  if i % 100 == 0:\n","    print(f'costo: {costo}')\n","\n","  with torch.no_grad(): \n","    pred_probab = nn.Softmax(dim=1)(mlp(X_train)) \n","    y_pred = pred_probab.argmax(1) \n","    # acc = average_precision_score(y_train, y_pred) \n","    # if i % 100 == 0:\n","    #   print(f'Accuracy: {acc}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"amUJ8DUb0zWK","executionInfo":{"status":"ok","timestamp":1678413097524,"user_tz":360,"elapsed":3140,"user":{"displayName":"Hiram Paulin","userId":"14403235101019868012"}},"outputId":"5381683e-a8a7-40a1-954f-be306a927fcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  input = module(input)\n"]},{"output_type":"stream","name":"stdout","text":["costo: 3.0880229473114014\n","costo: 3.0491151809692383\n","costo: 3.0044944286346436\n","costo: 3.006531238555908\n","costo: 2.9907784461975098\n","costo: 2.995671510696411\n","costo: 2.984285593032837\n","costo: 3.0149078369140625\n","costo: 2.996169090270996\n","costo: 3.0044312477111816\n"]}]},{"cell_type":"code","source":["def clean_text(text):\n","  tokens = nltk.word_tokenize(text)\n","  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","  return tokens\n","\n","def bag_of_words(text, voc):\n","  tok = clean_text(text)\n","  bow = [0] * len(voc)\n","  for w in tok:\n","    for i, w in enumerate(voc):\n","      if word == w:\n","        bow[i] = 1\n","  return np.array(bow)"],"metadata":{"id":"uroA30Ws1f1d"},"execution_count":null,"outputs":[]}]}